---
title: "Reasoning with Uncertainty"
subtitle: "An introduction to decision-making with data"
format: revealjs
editor_options: 
  chunk_output_type: console
---

## The Decision-Making Process 

Every decision involves a **decision maker** who must choose one **action** from a set of alternatives. Ultimately, the **outcome** of the decision maker's action will depend on the true **state of the world**. 

## You're the CHRO and You Have a Decision to Make

Recently, a competitor of yours was sued because their selection system was unintentionally discriminating against a protected class -- disparate impact. As the CHRO, you have to decide whether your selection system needs to be revised to ensure it's not causing disparate impact. 

What do you do?

## Deciding to Revise the Selection System

+----------------------+------------------------------------------------+------------------------------------------+
|                      | Disparate Impact                               | No Disparate Impact                      |
+:====================:+:==============================================:+:========================================:+
| Revise System        | Social & Ethical Benefit / Less legal exposure | Large expenses / Other opportunity costs |
+----------------------+------------------------------------------------+------------------------------------------+
| Keep System          | Law suits / Societal Disutility / Hurt org.    | Invest in other projects                 |
+----------------------+------------------------------------------------+------------------------------------------+

: {tbl-colwidths="[28, 36, 36]"}

## The Role of Data in Decision-Making

**When used appropriately**, data can be used to inform the decision-making process in many different ways: 

- Provide information about the most likely state
- Provide information about what actions to take and their impact
- Provide information about the outcomes that resulted from similar actions

## How Do We Use Data Appropriately? 

[Something about uncertainty here?] You will need to wrestle with questions around the significance and strength of an effect:

- Is the hiring rate of the majority group actually different from the hiring rate of the minority group?
- How much larger/smaller is the hiring rate of the majority group compared to that of the minority group? 

## Significance of an Effect

To answer questions about the significance of an effect, statisticians designed a statistical framework called Null Hypothesis Significance Testing (NHST). 

In our example, we want to know if the majority hiring rate ($\mathit{p}_{maj}$) is significantly different than the minority hiring rate ($\mathit{p}_{min}$).

## Components of the NHST Framework

1. Assumptions
2. Statistical Hypotheses
3. Test Statistic 
4. P-Value
5. Conclusion/Decision

## Statistical Assumptions

There are more than a few. 

They are often violated. 

Thank the mathematical powers that be that these tests are fairly insensitive to most violations. 

If you want to make a data scientist sweat, ask them if they checked the model assumptions. 

## Statistical Hypotheses 

A statistical hypothesis is a precise statement about the population effect you are estimating that should logically connect to the decision being made.

- Null Hypothesis ($H_0$): A statement that the effect takes a specific value---usually 0.
- Alternative Hypothesis ($H_a$): A statement that the effect takes on a range of values---usually any value, but 0.

## Setting Up the Null & Alternative Hypotheses 

The effect we are interested in is the difference in hiring rates: $\mathit{p}_{maj} - \mathit{p}_{min}$

- $H_0$: The majority group hiring rate is equal to the minority group hiring rate or $\mathit{p}_{maj} - \mathit{p}_{min}=0$.

- $H_a$: The majority group hiring rate does not equal the minority group hiring rate or $\mathit{p}_{maj} - \mathit{p}_{min} \neq 0$

## Building a Test Statistic

$$Z = \frac{\overbrace{(\mathit{p_{maj}} - \mathit{p_{min}})}^\text{Effect Est.} - \overbrace{0}^\text{Null Hyp.}}{\text{SE of Null Dist.}}$$

## Sampling Distribution of a Statistic

```{r}
#| label: sampling-distribution

set.seed(432)
p <- .7
q <- 1-p
se <- sqrt(p*q/500)
samp_dist <- 
  tibble::tibble(
    x = rnorm(1000, mean = p, sd = se)
    )

ggplot2::ggplot(
  data = samp_dist,
  ggplot2::aes(
    x = x
  )
) +
  ggplot2::geom_histogram(
    ggplot2::aes(y = ggplot2::after_stat(density)),
    bins = 25,
    color = "black"
  ) + 
  ggplot2::geom_function(
    fun = dnorm,
    args = list(mean = p, sd = se)
  ) + 
  ggplot2::geom_segment(
    x = p,
    xend = p,
    y = 0,
    yend = dnorm(p, mean = p, sd = se),
    color = "red"
  )
```

## P-Value

The p-value is the probability of observing a test statistic as extreme or more---in either direction---than your test statistic given that the null hypothesis is true. 

```{r}
#| label: null-samp-dist

p_mean <- 0
p1 <- .30
p2 <- .40
n1 <- 200
n2 <- 200
p_pool <- (p1*n1 + p2*n2) / (n1 + n2)
q_pool <- 1 - p_pool
p_var <- p_pool * q_pool
p_se <- sqrt(p_var * (1/n1 + 1/n2))
z_u <- qnorm(.90, mean = 0, sd = 1)
z_l <- -qnorm(.90, mean = 0, sd = 1)


ggplot2::ggplot(
  data = NULL,
  ggplot2::aes(
    x = c(qnorm(.001, mean = 0, sd = 1), qnorm(.999, mean = 0, sd = 1))
  )
) + 
  ggplot2::geom_area(
    stat = "function",
    fun = dnorm,
    args = list(mean = 0, sd = 1),
    fill = "red",
    xlim = c(qnorm(.001, mean = 0, sd = 1), z_l)
  ) + 
  ggplot2::geom_area(
    stat = "function",
    fun = dnorm,
    args = list(mean = 0, sd = 1),
    fill = "blue",
    xlim = c(z_l, z_u)
  ) + 
  ggplot2::geom_area(
    stat = "function",
    fun = dnorm,
    args = list(mean = 0, sd = 1),
    fill = "red",
    xlim = c(z_u, qnorm(.999, mean = 0, sd = 1))
  )
  
```

## Getting Comfortable with the P-Value

To calculate a p-value, you must be comfortable makeing the following assumptions:

1. The Null Distribution is true (weird, right?)
2. The sampling distribution of the test statistic is normal with a mean equal to the value specified by your null hypothesis and a standard deviation equal to the standard error under the null hypothesis

## Conclusion: Should I Reject my Null Hypothesis? 

You can think of the p-value as providing **evidence against the null hypothesis.** The smaller the p-value, the more evidence we have that the null hypothesis is not actually true. 

But how much evidence do we need? 

## Enter Alpha---the Significance Level

Generally, the person analyzing the data has either explicitly stated or implicitly assumed a threshold that if the p-value falls under, then they have sufficient evidence to **reject the null hypothesis.**

The value of this threshold is called $\alpha$ and it is usually set at .05. 

```{r}
#| label: alpha-plot

ggplot2::ggplot(
  data = NULL,
  ggplot2::aes(
    x = c(qnorm(.001, mean = 0, sd = 1), qnorm(.999, mean = 0, sd = 1))
  )
) + 
  ggplot2::geom_area(
    stat = "function",
    fun = dnorm,
    args = list(mean = 0, sd = 1),
    fill = "red",
    xlim = c(qnorm(.001, mean = 0, sd = 1), qnorm(.025))
  ) + 
  ggplot2::geom_area(
    stat = "function",
    fun = dnorm,
    args = list(mean = 0, sd = 1),
    fill = "blue",
    xlim = c(qnorm(.025), qnorm(.975))
  ) + 
  ggplot2::geom_area(
    stat = "function",
    fun = dnorm,
    args = list(mean = 0, sd = 1),
    fill = "red",
    xlim = c(qnorm(.975), qnorm(.999, mean = 0, sd = 1))
  )
  
```

## Does the NHST Framework Work? 

An article titled: *"Abandon statistical significance"* has been cited 1,004 times since it was published in 2019---which is a lot. 

## When Does the NHST Framework Fall Apart? 

When the data you are working with can be characterized as: 

- Noisy: A lot of measurement error
- Weak: Small effect sizes 
- Small: The number of observations (per group) is "small"

What does this mean about people data? 

## What Happens when NHST Falls Apart?

We start making statistical errors:

- **Statistical Decision Errors**: Type 1 & Type 2 Errors
- **Effect Size Errors**: Type S & Type M Errors

## Statistical Decision Errors

Statistical decision errors are made when we choose the wrong action based on our p-value:

+---------------------+--------------+--------------+
|                     | $H_0$ True   | $H_0$ False  |
+:===================:+:============:+:============:+
| Reject $H_0$        | Type 1 Error | Correct Dec. |
+---------------------+--------------+--------------+
| Do Not Reject $H_0$ | Correct Dec. | Type 2 Error |
+---------------------+--------------+--------------+

: {tbl-colwidths="[33, 33, 33]"}

## Effect Size Errors

Effect size errors are errors that are made when an estimated effect, deemed significant, is **quite** different from the true effect: 

- **Type S (Sign) Error**: The sign of a significant, estimated effect is opposite of the true effect
- **Type M (Magnitude) Error**: The value of a significant, estimated effect is far from the magnitude of the true effect

## Lets See Some Examples

## When Data Misleads You

How do these statistical errors impact your decision to revise your organization's selection system? 

- Type 1 Error: Spend money on unnecessary updates 
- Type 2 Error: Continued discrimination / legal exposure
- Type S Error: Continued discrimination / legal exposure / different interventions
- Type M Error: Unnecessary sense of alarm which could result in taking the wrong actions

## Methods to Understand Error Rates

Conduct a **design analysis**:

- **Prospective Design Analysis**: Before undertaking your analysis, simulate your data and calculate error rates under a range of different settings
- **Retrospective Design Analysis**: After undertaking your analysis, simulate similar data using parameters that mirror your study

## Methods to Reduce Error Rates

**Power** is the probability of rejecting $H_0$ when it is false. By increasing the power of your study, you are able to reduce Type 2, Type 2, and Type M error rates. To increase power, you can: 

- Increase your sample size
- Increase the strength of the effect size
- Decrease the $\alpha$ level

## Let's Just Get the Biggest Sample Possible

There are a few issues with this: 

- It can be pretty expensive and time consuming
- If you are comparing groups, then you need to make sure that you increase the individual group sizes
- With a large enough sample, you can find the tiniest effects to be significant

## Sample Size, Power, & Error Rates 

```{r}
#| label: power-analysis-plot

data_power <- 
  tibble::tibble(
    n_min = seq(1, 2000, by = 10)
  ) |>
  tidyr::expand_grid(
    tibble::tibble(
      effect = c(.04, .12, .21)
    )
  ) |>
  dplyr::mutate(
    n_maj = 4000,
    p_min = .30,
    p_maj = effect + p_min,
    p_pool = (p_maj * n_maj + p_min * n_min) / (n_maj + n_min),
    se_pool = sqrt((p_pool * (1 - p_pool)) * (1/n_maj + 1/n_min)),
    se_maj = sqrt((p_maj * (1 - p_maj)) / n_maj),
    se_min = sqrt((p_min * (1 - p_min)) / n_min),
    se_alt = sqrt(se_maj^2 + se_min^2),
    power = pnorm(qnorm(.975)*(se_pool / se_alt) - (effect / se_alt), lower.tail = F) + pnorm(-qnorm(.975)*(se_pool / se_alt) - (effect / se_alt)),
    s_error = pnorm(-qnorm(.975)*(se_pool / se_alt) - (effect / se_alt)) / power
  )

ggplot2::ggplot(
  data = data_power, 
  ggplot2::aes(
    x = n_min,
    y = power
  )
) + 
  ggplot2::geom_point() +
  ggplot2::facet_grid(. ~ effect)
```





